{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model_quantized import load_quantized_model, test_quantized_model, verify_model_state\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7bd5ac",
   "metadata": {},
   "source": [
    "# Load base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "822555fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path  = os.path.join(os.getcwd(), \"my-llama-3.1-8b-local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "963d047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Model from: /mnt/batch/tasks/shared/LS_root/mounts/clusters/apolloml-apac/code/Users/nontaporn.wonglek/LLM/my-llama-3.1-8b-local ===\n",
      "=== Setting up Quantization Configuration ===\n",
      "Quantization configs ready\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with 4-bit quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [03:42<00:00, 55.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model loaded successfully!\n",
      "GPU memory used: 5.31 GB / 15.6 GB\n",
      "Quantization: 4-bit\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_quantized_model(model_path, use_4bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18c75d",
   "metadata": {},
   "source": [
    "Test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "891ee893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Quantized Model ===\n",
      "Generating response...\n",
      "\n",
      "✅ Model test successful!\n",
      "Input: Hello! Can you tell me what is machine learning?\n",
      "Response: Hello! Can you tell me what is machine learning? Machine learning is a subset of artificial intelligence (AI) that involves training algorithms to make predictions, classify data, or make decisions based on patterns in data. It's a type of learning where machines can learn from data without being explicitly programmed. There are\n"
     ]
    }
   ],
   "source": [
    "test_success = test_quantized_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fe7bab",
   "metadata": {},
   "source": [
    "Check whether model is quantized properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "779c43c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Verification ===\n",
      "Model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "Model device: cuda:0\n",
      "Model dtype: torch.float16\n",
      "✅ Model is quantized with BitsAndBytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_model_state(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab830b7",
   "metadata": {},
   "source": [
    "# Prepare data and cross-check quality for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4dc206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"/home/azureuser/cloudfiles/code/Users/nontaporn.wonglek/LLM/data/llama_marketing_training.jsonl\"\n",
    "\n",
    "from data_prep import prepare_your_data_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c0cf7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 2: Preparing Training Data (JSONL) ===\n",
      "✅ Loaded 14 examples from /home/azureuser/cloudfiles/code/Users/nontaporn.wonglek/LLM/data/llama_marketing_training.jsonl\n",
      "✅ Formatted 14 examples\n",
      "\n",
      "=== Sample Formatted Text ===\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a marketing expert who creates engaging marketing content.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Write marketing content about: How Content Audits Help The...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 161.89 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenized dataset created: 14 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = prepare_your_data_jsonl(data_path, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f94481",
   "metadata": {},
   "source": [
    "# Set up LoRA for fine-tuning quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lora_finetune import setup_lora_pipeline, setup_training, complete_finetuning_pipeline, quick_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1c1dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LoRA for fine-tuning...\n",
      "=== Step 3: Preparing Model for LoRA ===\n",
      "✅ Model prepared for k-bit training\n",
      "✅ Gradient checkpointing enabled\n",
      "\n",
      "=== Creating LoRA Configuration ===\n",
      "✅ LoRA config created\n",
      "   - Rank: 8\n",
      "   - Alpha: 16\n",
      "   - Target modules: 7\n",
      "\n",
      "=== Applying LoRA to Model ===\n",
      "trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
      "\n",
      "GPU memory after LoRA: 7.36 GB / 15.6 GB\n",
      "\n",
      "LoRA setup complete\n",
      "✅ Model is ready for fine-tuning\n"
     ]
    }
   ],
   "source": [
    "peft_model = setup_lora_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a7f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training configuration...\n",
      "=== Step 4: Creating Training Configuration ===\n",
      "✅ Training arguments configured\n",
      "   - Epochs: 10\n",
      "   - Batch size: 1\n",
      "   - Learning rate: 0.0003\n",
      "   - Effective batch size: 4\n",
      "\n",
      "=== Creating Trainer ===\n",
      "✅ Trainer created successfully\n",
      "\n",
      "✅ Training setup complete\n",
      "✅ Current GPU memory: 7.36 GB / 15.6 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/apolloml-apac/code/Users/nontaporn.wonglek/LLM/Lora_finetune.py:149: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = setup_training(peft_model, tokenizer, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76afc306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting complete fine-tuning pipeline...\n",
      "Starting Fine-tuning\n",
      "==================================================\n",
      "Training in progress...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 04:57, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.749500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.482900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.845700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.517200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.421100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.387600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.118400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.914200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.943800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.819300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.624200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.625800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.586900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.515400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.574700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Fine-tuning completed successfully!\n",
      "Training time: 5.1 minutes\n",
      "Final GPU memory: 7.52 GB / 15.6 GB\n",
      "\n",
      "Saving fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to: ./marketing_lora_finetuned\n",
      "✅ LoRA adapters and tokenizer saved\n",
      "\n",
      "FINE-TUNING COMPLETE\n",
      "✅ Your marketing content generator is ready!\n",
      "✅ Saved at: ./marketing_lora_finetuned\n"
     ]
    }
   ],
   "source": [
    "success = complete_finetuning_pipeline(trainer, peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "144dd9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Quick Test: Write marketing content about: How AI is Revolutionizing Customer Experience\n",
      "--------------------------------------------------\n",
      "🧪 Testing Your Marketing Content Generator\n",
      "============================================================\n",
      "\n",
      "🎯 Test 1: Write marketing content about: How AI is Revolutio...\n",
      "------------------------------------------------------------\n",
      "📝 Generated Content:\n",
      "\n",
      "As a marketer, you know that customer experience is crucial to your business’s success. But delivering exceptional customer experiences is hard. And it’s getting harder. Customers have higher expectations than ever before, and they want to be able to resolve issues on their own. But as a business, you still want to provide them with personalized support that feels human. That’s where AI comes in. By integrating AI into your customer service strategy, you can deliver faster, more personalized su...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "quick_test(peft_model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
