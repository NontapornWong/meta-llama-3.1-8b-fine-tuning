{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcbae92",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bed5acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3861fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"marketing_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "raw_data = []\n",
    "cleaned_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3fe018",
   "metadata": {},
   "source": [
    "# Data Extraction\n",
    "\n",
    "Gets data from URL by pretending to be a real browser. The function waits 1 second before making the request to be polite to the website. It tries to download the webpage for up to 10 seconds. If there's an SSL certificate problem, it tries again without checking the certificate. If anything goes wrong, it returns None instead of crashing so your data collection can continue with other URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e946d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url, delay=1, verify_ssl=True):\n",
    "    \"\"\"Get a web page with basic error handling and SSL options\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        time.sleep(delay)  # Be polite\n",
    "        response = requests.get(url, headers=headers, timeout=10, verify=verify_ssl)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except requests.exceptions.SSLError as e:\n",
    "        print(f\"SSL Error for {url}. Trying without SSL verification...\")\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except Exception as e2:\n",
    "            print(f\"Error fetching {url} even without SSL verification: {e2}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "883044c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<!doctype html><html lang=\"en-us\"><head>\n",
      "    <meta charset=\"utf-8\">\n",
      "    <title>The HubSpot Marketing Blog </title>\n",
      "    <link rel=\"shortcut icon\" href=\"https://www.hubspot.com/hubfs/HubSpot_Logos/HubSpot-Inversed-Favicon.png\">\n",
      "    <!-- Primary Meta Description for Google Search Results -->\n",
      "    <meta name=\"description\" content=\"HubSpot’s Marketing Blog – attracting over 4.5 million monthly readers – covers everything you need to know to master inbound marketing.\">\n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "    <link rel=\"\n"
     ]
    }
   ],
   "source": [
    "url = \"https://blog.hubspot.com/marketing\"\n",
    "\n",
    "response = get_page(url, verify_ssl=False)\n",
    "\n",
    "print(response.status_code if response else \"Failed\")\n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07caca27",
   "metadata": {},
   "source": [
    "Creates a BeautifulSoup object from the downloaded webpage HTML content using the html parser. Finds the title tag from the webpage which contains the page title. Prints the actual title text without the HTML tags to see what the webpage is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "201a8dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HubSpot Marketing Blog \n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "title = soup.find('title')\n",
    "print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a823191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response length: 366416\n",
      "First 1000 characters:\n",
      "<!doctype html><html lang=\"en-us\"><head>\n",
      "    <meta charset=\"utf-8\">\n",
      "    <title>The HubSpot Marketing Blog </title>\n",
      "    <link rel=\"shortcut icon\" href=\"https://www.hubspot.com/hubfs/HubSpot_Logos/HubSpot-Inversed-Favicon.png\">\n",
      "    <!-- Primary Meta Description for Google Search Results -->\n",
      "    <meta name=\"description\" content=\"HubSpot’s Marketing Blog – attracting over 4.5 million monthly readers – covers everything you need to know to master inbound marketing.\">\n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "    <link rel=\"preload\" href=\"https://53.fs1.hubspotusercontent-na1.net/hubfs/53/tools/fonts/LexendDeca-Medium.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n",
      "    <link rel=\"preload\" href=\"https://53.fs1.hubspotusercontent-na1.net/hubfs/53/tools/fonts/LexendDeca-SemiBold.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n",
      "    <link rel=\"preload\" href=\"https://53.fs1.hubspotusercontent-na1.net/hubfs/53/tools/fonts/LexendDeca-Light.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response length: {len(response.text)}\")\n",
    "print(f\"First 1000 characters:\")\n",
    "print(response.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdcfad1",
   "metadata": {},
   "source": [
    "Takes the downloaded webpage HTML and searches for article link patterns using regular expressions. Looks specifically for URLs that match the HubSpot marketing blog pattern. Removes duplicate URLs by converting to a set then back to a list. Prints how many article URLs were found and displays the first 10 URLs with numbers so you can see what articles are available to scrape for your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a97cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for article patterns...\n",
      "Found 31 potential article URLs:\n",
      "1. https://blog.hubspot.com/marketing/page/183\n",
      "2. https://blog.hubspot.com/marketing/page/2\n",
      "3. https://blog.hubspot.com/marketing/resignation-letter\n",
      "4. https://blog.hubspot.com/marketing/communications-plan\n",
      "5. https://blog.hubspot.com/marketing/what-is-your-greatest-weakness\n",
      "6. https://blog.hubspot.com/marketing/marketing-plan-examples\n",
      "7. https://blog.hubspot.com/marketing/rss.xml\n",
      "8. https://blog.hubspot.com/marketing/how-to-use-instagram\n",
      "9. https://blog.hubspot.com/marketing/digital-strategy-guide\n",
      "10. https://blog.hubspot.com/marketing/gain-instagram-followers\n"
     ]
    }
   ],
   "source": [
    "# Search for common article patterns in the HTML\n",
    "text = response.text\n",
    "print(\"Searching for article patterns...\")\n",
    "\n",
    "# Look for URLs that might be articles\n",
    "article_urls = re.findall(r'https://blog\\.hubspot\\.com/marketing/[^\"\\'>\\s]+', text)\n",
    "article_urls = list(set(article_urls))  # Remove duplicates\n",
    "\n",
    "print(f\"Found {len(article_urls)} potential article URLs:\")\n",
    "for i, url in enumerate(article_urls[:10]):\n",
    "    print(f\"{i+1}. {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45561b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 real article URLs:\n",
      "1. https://blog.hubspot.com/marketing/resignation-letter\n",
      "2. https://blog.hubspot.com/marketing/communications-plan\n",
      "3. https://blog.hubspot.com/marketing/what-is-your-greatest-weakness\n",
      "4. https://blog.hubspot.com/marketing/marketing-plan-examples\n",
      "5. https://blog.hubspot.com/marketing/how-to-use-instagram\n",
      "6. https://blog.hubspot.com/marketing/digital-strategy-guide\n",
      "7. https://blog.hubspot.com/marketing/gain-instagram-followers\n",
      "8. https://blog.hubspot.com/marketing/top-search-engines\n",
      "9. https://blog.hubspot.com/marketing/professional-bio-examples\n",
      "10. https://blog.hubspot.com/marketing/marketing-strategy\n",
      "11. https://blog.hubspot.com/marketing/best-ai-chatbot\n",
      "12. https://blog.hubspot.com/marketing/loop-marketing\n",
      "13. https://blog.hubspot.com/marketing/how-to-repost-on-instagram\n",
      "14. https://blog.hubspot.com/marketing/why-creator-marketing-works-for-any-business-tips-from-a-creator-consultant\n",
      "15. https://blog.hubspot.com/marketing/i-tried-5-ai-logo-generators\n",
      "16. https://blog.hubspot.com/marketing/instagram-best-time-post\n",
      "17. https://blog.hubspot.com/marketing/what-makes-you-unique\n",
      "18. https://blog.hubspot.com/marketing/marketing-plan-template-generator\n",
      "19. https://blog.hubspot.com/marketing/funny-weird-interview-questions\n",
      "20. https://blog.hubspot.com/marketing/ai-avatar-tools\n",
      "21. https://blog.hubspot.com/marketing/hubspot-blog-marketing-industry-trends-report\n",
      "22. https://blog.hubspot.com/marketing/social-media-calendar-tools\n",
      "23. https://blog.hubspot.com/marketing/ai-for-graphic-design\n"
     ]
    }
   ],
   "source": [
    "# Filter for real articles (exclude pagination, RSS, etc.)\n",
    "real_articles = []\n",
    "for url in article_urls:\n",
    "    if not any(x in url for x in ['page/', 'rss.xml']):\n",
    "        real_articles.append(url)\n",
    "\n",
    "print(f\"Found {len(real_articles)} real article URLs:\")\n",
    "for i, url in enumerate(real_articles):\n",
    "    print(f\"{i+1}. {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1130ca",
   "metadata": {},
   "source": [
    "Takes the first article URL from the discovered list and tests if it can be downloaded. Uses the get_page function with SSL verification disabled to fetch the individual article. If successful, prints a checkmark and shows how many characters the article contains to verify it downloaded properly. If it fails, prints an X mark to indicate the article couldn't be retrieved for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89abea6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: https://blog.hubspot.com/marketing/resignation-letter\n",
      "✅ Got article!\n",
      "Length: 352714 characters\n"
     ]
    }
   ],
   "source": [
    "# Test getting content from one article\n",
    "test_url = real_articles[0]  # First article\n",
    "print(f\"Testing: {test_url}\")\n",
    "\n",
    "article_response = get_page(test_url, verify_ssl=False)\n",
    "if article_response:\n",
    "    print(\"✅ Got article!\")\n",
    "    print(f\"Length: {len(article_response.text)} characters\")\n",
    "else:\n",
    "    print(\"❌ Failed to get article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b64659d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: How Content Audits Help The HubSpot Blog Age Backwards — A Peek Into Our Process \n",
      "Content length: 20043 characters\n",
      "First 200 characters: In 2023, my team and I began working on perhaps one of the most ambitious content audits ever conducted on the HubSpot Blog. We’ve run content audits in the past — but not like this.\n",
      "\n",
      "We ran the audit\n"
     ]
    }
   ],
   "source": [
    "# Parse the article content\n",
    "article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "\n",
    "# Get title\n",
    "title = article_soup.find('title')\n",
    "print(f\"Title: {title.text if title else 'No title found'}\")\n",
    "\n",
    "# Get main content (try common content selectors)\n",
    "content_selectors = ['[class*=\"post-body\"]', '[class*=\"content\"]', 'main', 'article']\n",
    "content = None\n",
    "\n",
    "for selector in content_selectors:\n",
    "    content_elem = article_soup.select_one(selector)\n",
    "    if content_elem:\n",
    "        content = content_elem.get_text().strip()\n",
    "        break\n",
    "\n",
    "if content:\n",
    "    print(f\"Content length: {len(content)} characters\")\n",
    "    print(f\"First 200 characters: {content[:200]}\")\n",
    "else:\n",
    "    print(\"No content found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34ef92a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article data:\n",
      "- URL: https://blog.hubspot.com/marketing/resignation-letter\n",
      "- Title: How Content Audits Help The HubSpot Blog Age Backwards — A Peek Into Our Process \n",
      "- Word count: 3348\n",
      "- Content preview: In 2023, my team and I began working on perhaps one of the most ambitious content audits ever conduc...\n"
     ]
    }
   ],
   "source": [
    "# Save the article data\n",
    "article_data = {\n",
    "    'url': test_url,\n",
    "    'title': title.text if title else '',\n",
    "    'content': content,\n",
    "    'word_count': len(content.split()),\n",
    "    'scraped_date': '2025-09-05'\n",
    "}\n",
    "\n",
    "print(f\"Article data:\")\n",
    "print(f\"- URL: {article_data['url']}\")\n",
    "print(f\"- Title: {article_data['title']}\")\n",
    "print(f\"- Word count: {article_data['word_count']}\")\n",
    "print(f\"- Content preview: {article_data['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27b6b3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://blog.hubspot.com/marketing/resignation-letter',\n",
       " 'title': 'How Content Audits Help The HubSpot Blog Age Backwards —\\xa0A Peek Into Our Process ',\n",
       " 'content': \"In 2023, my team and I began working on perhaps one of the most ambitious content audits ever conducted on the HubSpot Blog. We’ve run content audits in the past — but not like this.\\n\\nWe ran the audit in three phases:\\n\\nPhase 1 addressed our oldest content.\\nPhase 2 evaluated our lowest-performing content.\\nPhase 3 assessed the value of our topic clusters.\\n\\nWhen it was all said and done, we audited over 10,000 blog post URLs and over 450 topic clusters.\\n\\nIn this post, I’m going to focus on phase one of our audit. I’ll walk you through how we audited our oldest content and how we took action. Plus, I’ll share the results we found.\\nBut first, let me give you some background on why we decided to run an audit of this magnitude.\\nWhy We Audited\\nIt all started in early 2023. At the time, my team was called the Historical Optimization team and we sat at the intersection of HubSpot’s SEO and Blog teams.\\nWe were responsible for updating and optimizing our existing blog posts and finding growth opportunities within our library. (We’ve since evolved into what is now the EN Blog Strategy team.)\\nIn case you’re new here, the HubSpot Blog is HUGE.\\nFor context, the blog was home to 13,822 pages in February of 2023, the month we began our audit. \\nWhile we are fortunate to have a high domain authority and drive millions of visits per month, having a blog of this size does not come without challenges.\\nAs our library ages, the amount of opportunity for new content across our blog properties and clusters shrinks.\\nSo, we decided to audit our library to find opportunities for optimization.\\nWe hypothesized that we could uncover “greenspace” and “quasi-greenspace” — topics that we have covered but haven’t capitalized on that well — by auditing the oldest 4,000 URLs in our library.\\nAlthough this was only about a third of our content library, we believed we’d be able to unearth some traffic opportunities and give our blog a boost.\\n\\n\\n\\n\\n\\n\\n\\n\\nHubSpot's Guide to Navigating EEAT in the AI Era\\nDiscover how HubSpot tackled SERP volatility and redefined its strategy to thrive in the new EEAT landscape. \\n\\n\\n\\n\\n\\n              Strategies for creating experience-rich & authoritative content\\n            \\n\\n\\n\\n\\n              Insights from HubSpot experts \\n            \\n\\n\\n\\n\\n              Practical tools to implement EEAT\\n            \\n\\nGet the Guide\\n\\n                Learn more\\n                \\n\\n\\n\\n\\n\\n                Get the Guide\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDownload FreeAll fields are required.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou're all set!\\nClick this link to access this resource at any time.\\n\\nDownload Now\\n\\n\\n\\n\\n\\n\\nAround the same time, we started to feel the effects of Google’s March 2023 Core Update that emphasized experience, which our Technical SEO team immediately started addressing.\\nHowever, another part of that algorithm update emphasized content freshness and helpfulness. In other words, how cutting-edge and useful our content is to our readers.\\nThis is where we really felt a sense of urgency.\\nBecause we had 4,000 URLs with published dates ranging from 2006 to 2015, we already knew that this chunk of content was not fresh or helpful.\\nSo, we got to work and audited those blog posts over the course of ten weeks.\\nEventually, we added phases two and three to our plan so we could further address unhelpful content and clusters.\\nNote: In addition to these content audits, we continually re-evaluate the relevance of past content strategies and prune posts that no longer serve our audience, such as our posts on Famous Quotes, How to Write a Resignation Letter, and How to Type the Shrug Emoji.\\nHow We Audited Our Oldest Content\\n\\n1. Define our goals.\\nBefore we started auditing the content, it was important for us to determine the objectives.\\nFor some publishers, the goal of a content audit may include improving on-page SEO, enhancing user engagement, aligning content with marketing goals, or identifying content gaps.\\nFor this particular audit, it meant uncovering “greenspace” and “quasi-greenspace” in our blog library, and improving our overall content freshness.\\nWe also had to determine the scope of our audit.\\nThere’s no right or wrong way to approach this. Depending on your goals and the size of your website, you could audit the whole thing in one go.\\nYou could also start with a small portion of your site (such as product pages or specific topic clusters) and build out from there.\\nSince HubSpot has such a large content library, we opted to limit this audit to our oldest 4,000 URLs. Not only was this more manageable than reviewing all of our content in one audit, but this also targeted URLs that were more likely to benefit from an update or prune.\\nWe also did this knowing that we would later address the rest of our library during phases two and three.\\n2. Gather our content inventory.\\nOnce we established our goals and scope, we had to gather the oldest 4,000 blog posts and put them into a spreadsheet.\\nThis process can vary depending on the tools and CMS you use. Here’s how we did it using Content Hub:\\n1. Log into HubSpot and navigate to the Blog page in Content Hub.\\n2. Navigate to the Actions drop-down and click Export blog posts.\\n3. Select File format and click Export. This will send all of your blog post information to your email. You will also get a notification in HubSpot once your export is ready.\\n4. Download your export and open it in your preferred spreadsheet software (I’m usually a Google Sheets girlie, but I had to use Microsoft Excel since the file was so large).\\n5. Review each column in the spreadsheet and delete the ones that are not relevant to your audit. We immediately deleted the following:\\n\\nPost SEO title\\nMeta description\\nLast modified date\\nPost body\\nFeatured image URL\\nHead HTML\\nArchived\\n\\n6. Once the irrelevant columns were removed, the following remained:\\n\\nBlog name\\nPost title\\nTags\\nPost language\\nPost URL\\nAuthor\\nPublish date\\nStatus\\n\\n7. Filter the Post language column for EN posts only. Delete the column once the sheet is filtered.\\n8. Filter the Status column for PUBLISHED only. Delete the column once the sheet is filtered.\\n9. Filter the sheet by Publish date from oldest to newest.\\n10. Highlight and copy the first 4,000 rows and paste them into a separate spreadsheet.\\n11. Name the new spreadsheet Content Audit Master.\\nIf you’re feeling fancy, you can also create a custom report in Content Hub and select only the fields you want included in the audit so you don’t have to filter as much when setting up your spreadsheet.\\n3. Retrieve the data.\\nAfter compiling all of the content needed for our audit, we had to collect relevant data for each blog post.\\nFor this audit, we kept it pretty simple, only analyzing total organic traffic from the previous calendar year, total backlinks, and total keywords.\\nWe did this because our recommended actions for each URL were determined during post-evaluation. (We’ll cover this in the next step.)\\nWe obtained organic traffic data from Google Search Console and used a VLOOKUP to match each URL with its corresponding number of Clicks.\\nThen, we got backlink and keyword data via a keyword research tool.\\n4. Evaluate the content.\\nNext, we assessed each piece of content by using the collected data. Then, we evaluated the post itself to determine the following:\\n\\nType of Content\\nFreshness Level\\nOrganic Potential\\n\\nType of Content\\nThe HubSpot Blog is home to many different types of blog posts, each serving a unique purpose. Labeling each post helped us determine its relevance and became a key factor in our decision to update or prune.\\nWhile this isn’t an exhaustive list of all the content types you could find on the HubSpot Blog, we narrowed it down to the following for the purposes of this audit:\\n\\nEducational: A topic that can educate the user on a pain point or problem they know they have.\\nThought Leadership: A topic that can educate the user on a pain point or problem they didn’t know they had until an expert drew it to their attention.\\nBusiness Update: A HubSpot-related piece of news or a press release that is likely not evergreen.\\nNewsjacking: An industry-related piece of news or a press release that is likely not evergreen.\\nResearch: A collection of data or the results of an experiment that is used to educate the reader. This topic may or may not be evergreen, but the content is not and needs updates to stay fresh.\\n\\nFreshness Level\\nBecause the posts in this audit hadn’t been updated in a long time, none of them could be considered 100% “fresh.” However, we took different types of freshness into account when determining what action needed to be taken on the URLs.\\nFor example, some topics, such as Google+, are so outdated that an update would be silly. However, plenty of topics were still evergreen, even though our content was not.\\nThe following scale helped us make decisions on whether the URL had value with regard to freshness:\\n\\nOutdated: The topic is outdated, and an update may not be possible.\\nStale: The topic is evergreen, but it would need an extensive update to make the content more competitive.\\nRelatively Fresh: The topic is evergreen, and it would only need a moderate update to make the content competitive.\\n\\nOrganic Potential\\nTo determine the organic potential of each URL, we had to ask ourselves the following question: Will anyone search for the content on Google?\\n\\nYes: Someone would definitely search for this, so we’ll need to optimize/recycle the content.\\nNo: Someone would not search for this. There’s no point in optimizing/recycling the content since there’s no possible focus keyword.\\n\\nFor all of the posts marked “Yes” for organic potential, we recommended a focus keyword for the re-optimized content to compete for. We did this by evaluating the existing title, slug, and content. Then, we did some keyword research and reviewed the Google SERP for that query.\\nWe also included the focus keyword’s monthly search volume (MSV) to help prioritize which updates to perform first.\\xa0\\nFor an extra layer of caution, we also checked for cannibalization on all posts marked “Yes” for organic potential. There are a few ways to do this:\\n\\nDo a site search and see if any URLs come up for the focus keyword.\\nUse a keyword research tool to cross-reference whether any other posts on your site are ranking. (This is the fastest/most accurate method.)\\n\\nIf the focus keyword was flagged for cannibalization, we either found a different focus keyword or noted that the URL should be redirected to the fresher post.\\nIf no cannibalization was found, then we had the green light to move forward with updating the post.\\n5. Recommend an action.\\nOnce a post was completely evaluated, we turned the insights into action items.\\nEach URL was placed into one of the following categories:\\n\\nKeep: No action is needed because both the content and the URL are good.\\nOptimize: The content is good but outdated in terms of freshness or SEO practices. Keep the spirit of the article, but refresh and re-optimize to improve performance.\\nRecycle: The content is not salvageable, but the URL still has value (in terms of backlinks or keyword opportunity). Create new content from scratch, but retain the URL.\\nPrune: Neither the content nor the URL has value from an organic standpoint.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHubSpot's Guide to Navigating EEAT in the AI Era\\nDiscover how HubSpot tackled SERP volatility and redefined its strategy to thrive in the new EEAT landscape. \\n\\n\\n\\n\\n\\n              Strategies for creating experience-rich & authoritative content\\n            \\n\\n\\n\\n\\n              Insights from HubSpot experts \\n            \\n\\n\\n\\n\\n              Practical tools to implement EEAT\\n            \\n\\nGet the Guide\\n\\n                Learn more\\n                \\n\\n\\n\\n\\n\\n                Get the Guide\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDownload FreeAll fields are required.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou're all set!\\nClick this link to access this resource at any time.\\n\\nDownload Now\\n\\n\\n\\n\\n\\n\\nAudit Insights\\nOut of the 4,000 URLs we audited, 951 (23.78%) were categorized as posts with organic potential and recommended for optimization or recycling. Additionally, 2,888 URLs were recommended to be pruned. That’s about 72.2% of the audit.\\nThese posts either did not have organic potential, posed a cannibalization risk, or were so outdated that there was no point in updating them.\\nThe remaining 161 URLs either did not require any action or had already been redirected.\\nHow We Took Action\\nThe action taken for a URL was determined by its potential for organic traffic.\\nThe URLs with organic potential were delivered to our Blog team and recommended to be optimized or recycled.\\nMeanwhile, the URLs with no organic potential were delivered to our SEO team and recommended to be archived or redirected.\\nFirst, let’s walk through how we took action on the posts recommended to be optimized or recycled.\\nTaking Action on Content with Organic Potential\\nBefore addressing any of the 951 posts with organic potential, we needed to figure out the following:\\n\\nOur capacity for strategic analysis and brief writing\\nThe capacity of our in-house writing staff and available freelancers\\nOur capacity to edit the updates\\n\\nWe coordinated with stakeholders and determined we only had the bandwidth to update 240 posts in 2024 (in addition to the dozens of blog posts we update each month). This initiative was internally known as the “De-Age the Blog Project” and was led by my EN Blog Strategy teammate Kimberly Turner.\\nOnce we knew how many posts we could take on, we had to narrow down which ones to prioritize. We did this by evaluating the complexity of the lift required for each post update:\\n\\nSimple Update: The content updates needed are relatively light, making them suitable for freelancers.\\nComplex Update: The content updates needed are heavy, making them better suited for in-house writers.\\nRecycle: Content is not salvageable, but the URL is. Rewrite the post from scratch, but retain the URL.\\nNo Opportunity: Pass on updating.\\n\\nWe originally prioritized updating the simplest URLs first, but later pivoted our strategy to tackle the URLs with the highest MSV potential, regardless of update complexity.\\nWe did this because we wanted to get the most we could out of our updates.\\nDe-Age the Blog Results\\nInitially, we projected that these updates would be complete by the end of H1 2024, but we had to shift our strategy … again.\\nLike many other publishers, we felt the effects of Google’s March 2024 Core Update as well as the introduction of AI Overviews.\\nAfter having placed the De-Age the Blog Project on hold while we addressed the issues, we deprioritized the project entirely in favor of higher-impact workstreams.\\nSEO, am I right? It always keeps you on your toes.\\nDespite sunsetting the project before it was complete, we were still able to perform 76 post updates. Six months after the updates were implemented, the cumulative monthly traffic for these posts had increased by 458%.\\nThis goes to show that even updating a small portion of URLs can make a big difference.\\nTaking Action on Content with No Organic Potential\\nWhile the De-Age the Blog Project was taking place, we also took action on the 2,888 URLs that were recommended to be pruned.\\nSince the initial audit didn’t include recommendations on how to prune, we had to go back and re-review each URL to determine how we would prune.\\nHere’s how we evaluated the posts:\\n\\nArchive (404): The URL has less than 10 backlinks and the backlink profile does not have value.\\nRedirect (301): The URL has more than 10 backlinks and/or the backlink profile has value.\\n\\nWhen choosing a new URL, we did our best to pick the most relevant and similar page. If we couldn’t find one, we redirected to the pillar page of the cluster that the post belonged to.\\nIf for some reason, the URL didn’t belong to a cluster or there wasn’t a pillar page, we redirected it to the HubSpot Blog homepage.\\nDecision-making for some content types was easier than others. For example, we were able to automatically assign 301 redirects to URLs that were flagged for cannibalization during the initial audit. We also automatically assigned 404s to URLs with less than 10 backlinks labeled as Newsjacking and Business Updates.\\nEverything else was manually reviewed to ensure accuracy. To make the evaluation process easier, we followed this decision tree:\\n\\nIt took my team about two and a half weeks to ensure that every URL had the correct label. In the end, we had 1,675 URLs assigned to be redirected and 1,210 URLs assigned to be unpublished and archived.\\nOnce each URL was evaluated, we were finally ready to take action.\\nAfter coordinating with Rory and Principal Technical SEO Strategist, Sylvain Charbit, we decided to prune the URLs in batches instead of all at once. That way, we could better monitor the impact of redirecting and archiving a large quantity of content.\\nOriginally, we planned to implement our prune in five batches over five weeks, allowing us time to monitor performance during the weeks in between.\\nBatches one and two contained URLs meant to be archived and unpublished, and batches three through five contained URLs designated for 301 redirects.\\nBecause there were so many URLs to unpublish and archive, we worked with developers on HubSpot’s Digital Experience team to create a script that would automatically unpublish and archive URLs and redirect them to our 404 page.\\nThen, we were able to implement the 301 redirects with the Bulk URL Redirect tool in Content Hub.\\nNote: Although we were able to work through this process internally and finish before our deadline, I want to acknowledge that manually evaluating over 2,000 URLs can be tedious and time-consuming.\\nDepending on your resources and the scope of your audit, you may want to consider hiring a freelancer to help your team work through a task this large.\\nContent Pruning Results\\nWhile we successfully implemented each batch, this process didn’t come without a few road bumps.\\nMidway through our pruning schedule, Google rolled out the March 2024 Core Algorithm Update. We ended up placing our pruning schedule on hold so we could better monitor performance during the update.\\nOnce the update was complete, we resumed the rest of our prune until it was complete.\\nBecause of the volatile search landscape in 2024, we didn’t see the traffic gains we’d hoped to see once the prune was complete. However, we did celebrate a massive win for overall content freshness on the blog.\\nAt the start of our audit in 2023, we calculated the freshness of our content library by looking at each URL's publish date and quantifying the number of days since they were updated.\\nFor example, say the current date is November 12, 2024, and you have a post that was last updated on February 19, 2008. Based on the 2024 date, the post from 2008 is 16.7 years old or 6,110 days.\\nOnce we had all of the ages for every post on the HubSpot Blog, we averaged those numbers to determine the average age of our content library, which was 2,088 days (5.7 years).\\nSince pruning 2,888 URLs (and updating hundreds of URLs from the audit and beyond), the HubSpot Blog's average age has dropped to 1,747 days — that’s 341 days younger than when we started.\\nAs content freshness and helpfulness play an even greater role in search algorithms, being nearly a year younger can make a big difference.\\nWhat’s Next?\\nEarlier in this post, I mentioned that this audit is only one of three that my team has worked on in 2024.\\nOur phase two audit focuses on the lowest-performing posts that were not included in phase one, totaling over 6000 URLs. Then, phase three assesses the value of our Blog’s topic clusters.\\nWe’re still taking action on the results from these audits, but I’m so excited to share the process and insights when they’re complete.\\nUltimately, content auditing is a job that is never truly done — especially when working with large libraries. You finish one audit, then it’s on to the next.\\nAlthough the work can be tedious, the rewards of improving content quality, user experience, and performance make it worth the effort.\\n\\n\\nTopics:\\nSEO\\n\\n\\n\\nDon't forget to share this post!\",\n",
       " 'word_count': 3348,\n",
       " 'scraped_date': '2025-09-05'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a107e",
   "metadata": {},
   "source": [
    "Take all articals and keep it i list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0fe57",
   "metadata": {},
   "source": [
    "Creates an empty list to store all article data. Goes through each article URL one by one and downloads it using the get_page function. For each successful download, it extracts the title and tries different methods to find the main article content. Only saves articles that have more than 500 characters of content. Stores each article with its URL, title, content text, and word count in a dictionary then adds it to the main list. Prints progress for each article showing success or failure. Finally displays the total number of articles successfully collected for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e706758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping article 1/23: https://blog.hubspot.com/marketing/resignation-letter\n",
      "✅ Saved! Words: 3348\n",
      "Scraping article 2/23: https://blog.hubspot.com/marketing/communications-plan\n",
      "✅ Saved! Words: 4468\n",
      "Scraping article 3/23: https://blog.hubspot.com/marketing/what-is-your-greatest-weakness\n",
      "✅ Saved! Words: 3348\n",
      "Scraping article 4/23: https://blog.hubspot.com/marketing/marketing-plan-examples\n",
      "✅ Saved! Words: 6905\n",
      "Scraping article 5/23: https://blog.hubspot.com/marketing/how-to-use-instagram\n",
      "✅ Saved! Words: 5302\n",
      "Scraping article 6/23: https://blog.hubspot.com/marketing/digital-strategy-guide\n",
      "✅ Saved! Words: 10776\n",
      "Scraping article 7/23: https://blog.hubspot.com/marketing/gain-instagram-followers\n",
      "✅ Saved! Words: 6355\n",
      "Scraping article 8/23: https://blog.hubspot.com/marketing/top-search-engines\n",
      "✅ Saved! Words: 4675\n",
      "Scraping article 9/23: https://blog.hubspot.com/marketing/professional-bio-examples\n",
      "✅ Saved! Words: 8403\n",
      "Scraping article 10/23: https://blog.hubspot.com/marketing/marketing-strategy\n",
      "✅ Saved! Words: 5605\n",
      "Scraping article 11/23: https://blog.hubspot.com/marketing/best-ai-chatbot\n",
      "✅ Saved! Words: 7272\n",
      "Scraping article 12/23: https://blog.hubspot.com/marketing/loop-marketing\n",
      "✅ Saved! Words: 1578\n",
      "Scraping article 13/23: https://blog.hubspot.com/marketing/how-to-repost-on-instagram\n",
      "✅ Saved! Words: 3624\n",
      "Scraping article 14/23: https://blog.hubspot.com/marketing/why-creator-marketing-works-for-any-business-tips-from-a-creator-consultant\n",
      "✅ Saved! Words: 895\n",
      "Scraping article 15/23: https://blog.hubspot.com/marketing/i-tried-5-ai-logo-generators\n",
      "✅ Saved! Words: 2724\n",
      "Scraping article 16/23: https://blog.hubspot.com/marketing/instagram-best-time-post\n",
      "✅ Saved! Words: 6154\n",
      "Scraping article 17/23: https://blog.hubspot.com/marketing/what-makes-you-unique\n",
      "✅ Saved! Words: 3260\n",
      "Scraping article 18/23: https://blog.hubspot.com/marketing/marketing-plan-template-generator\n",
      "✅ Saved! Words: 5433\n",
      "Scraping article 19/23: https://blog.hubspot.com/marketing/funny-weird-interview-questions\n",
      "✅ Saved! Words: 2633\n",
      "Scraping article 20/23: https://blog.hubspot.com/marketing/ai-avatar-tools\n",
      "✅ Saved! Words: 1125\n",
      "Scraping article 21/23: https://blog.hubspot.com/marketing/hubspot-blog-marketing-industry-trends-report\n",
      "✅ Saved! Words: 4158\n",
      "Scraping article 22/23: https://blog.hubspot.com/marketing/social-media-calendar-tools\n",
      "✅ Saved! Words: 6745\n",
      "Scraping article 23/23: https://blog.hubspot.com/marketing/ai-for-graphic-design\n",
      "✅ Saved! Words: 3731\n",
      "\n",
      "Total articles collected: 23\n"
     ]
    }
   ],
   "source": [
    "all_articles = []\n",
    "\n",
    "for i, url in enumerate(real_articles, 1):\n",
    "    print(f\"Scraping article {i}/23: {url}\")\n",
    "\n",
    "    response = get_page(url, verify_ssl=False)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.find('title')\n",
    "        title_text = title.text if title else ''\n",
    "\n",
    "        content_selectors = ['[class*=\"post-body\"]', '[class*=\"content\"]', 'main', 'article']\n",
    "        content = None\n",
    "        for selector in content_selectors:\n",
    "            content_elem = soup.select_one(selector)\n",
    "            if content_elem:\n",
    "                content = content_elem.get_text().strip()\n",
    "                break\n",
    "\n",
    "        if content and len(content) > 500:\n",
    "            article_data = {\n",
    "                'url': url,\n",
    "                'title': title_text,\n",
    "                'content': content,\n",
    "                'word_count': len(content.split())\n",
    "            }\n",
    "            all_articles.append(article_data)\n",
    "            print(f\"✅ Saved! Words: {article_data['word_count']}\")\n",
    "        else:\n",
    "            print(\"❌ No content found\")\n",
    "    else:\n",
    "        print(\"❌ Failed to fetch\")\n",
    "\n",
    "print(f\"\\nTotal articles collected: {len(all_articles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70bfee59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://blog.hubspot.com/marketing/resignation-letter',\n",
       " 'title': 'How Content Audits Help The HubSpot Blog Age Backwards —\\xa0A Peek Into Our Process ',\n",
       " 'content': \"In 2023, my team and I began working on perhaps one of the most ambitious content audits ever conducted on the HubSpot Blog. We’ve run content audits in the past — but not like this.\\n\\nWe ran the audit in three phases:\\n\\nPhase 1 addressed our oldest content.\\nPhase 2 evaluated our lowest-performing content.\\nPhase 3 assessed the value of our topic clusters.\\n\\nWhen it was all said and done, we audited over 10,000 blog post URLs and over 450 topic clusters.\\n\\nIn this post, I’m going to focus on phase one of our audit. I’ll walk you through how we audited our oldest content and how we took action. Plus, I’ll share the results we found.\\nBut first, let me give you some background on why we decided to run an audit of this magnitude.\\nWhy We Audited\\nIt all started in early 2023. At the time, my team was called the Historical Optimization team and we sat at the intersection of HubSpot’s SEO and Blog teams.\\nWe were responsible for updating and optimizing our existing blog posts and finding growth opportunities within our library. (We’ve since evolved into what is now the EN Blog Strategy team.)\\nIn case you’re new here, the HubSpot Blog is HUGE.\\nFor context, the blog was home to 13,822 pages in February of 2023, the month we began our audit. \\nWhile we are fortunate to have a high domain authority and drive millions of visits per month, having a blog of this size does not come without challenges.\\nAs our library ages, the amount of opportunity for new content across our blog properties and clusters shrinks.\\nSo, we decided to audit our library to find opportunities for optimization.\\nWe hypothesized that we could uncover “greenspace” and “quasi-greenspace” — topics that we have covered but haven’t capitalized on that well — by auditing the oldest 4,000 URLs in our library.\\nAlthough this was only about a third of our content library, we believed we’d be able to unearth some traffic opportunities and give our blog a boost.\\n\\n\\n\\n\\n\\n\\n\\n\\nHubSpot's Guide to Navigating EEAT in the AI Era\\nDiscover how HubSpot tackled SERP volatility and redefined its strategy to thrive in the new EEAT landscape. \\n\\n\\n\\n\\n\\n              Strategies for creating experience-rich & authoritative content\\n            \\n\\n\\n\\n\\n              Insights from HubSpot experts \\n            \\n\\n\\n\\n\\n              Practical tools to implement EEAT\\n            \\n\\nGet the Guide\\n\\n                Learn more\\n                \\n\\n\\n\\n\\n\\n                Get the Guide\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDownload FreeAll fields are required.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou're all set!\\nClick this link to access this resource at any time.\\n\\nDownload Now\\n\\n\\n\\n\\n\\n\\nAround the same time, we started to feel the effects of Google’s March 2023 Core Update that emphasized experience, which our Technical SEO team immediately started addressing.\\nHowever, another part of that algorithm update emphasized content freshness and helpfulness. In other words, how cutting-edge and useful our content is to our readers.\\nThis is where we really felt a sense of urgency.\\nBecause we had 4,000 URLs with published dates ranging from 2006 to 2015, we already knew that this chunk of content was not fresh or helpful.\\nSo, we got to work and audited those blog posts over the course of ten weeks.\\nEventually, we added phases two and three to our plan so we could further address unhelpful content and clusters.\\nNote: In addition to these content audits, we continually re-evaluate the relevance of past content strategies and prune posts that no longer serve our audience, such as our posts on Famous Quotes, How to Write a Resignation Letter, and How to Type the Shrug Emoji.\\nHow We Audited Our Oldest Content\\n\\n1. Define our goals.\\nBefore we started auditing the content, it was important for us to determine the objectives.\\nFor some publishers, the goal of a content audit may include improving on-page SEO, enhancing user engagement, aligning content with marketing goals, or identifying content gaps.\\nFor this particular audit, it meant uncovering “greenspace” and “quasi-greenspace” in our blog library, and improving our overall content freshness.\\nWe also had to determine the scope of our audit.\\nThere’s no right or wrong way to approach this. Depending on your goals and the size of your website, you could audit the whole thing in one go.\\nYou could also start with a small portion of your site (such as product pages or specific topic clusters) and build out from there.\\nSince HubSpot has such a large content library, we opted to limit this audit to our oldest 4,000 URLs. Not only was this more manageable than reviewing all of our content in one audit, but this also targeted URLs that were more likely to benefit from an update or prune.\\nWe also did this knowing that we would later address the rest of our library during phases two and three.\\n2. Gather our content inventory.\\nOnce we established our goals and scope, we had to gather the oldest 4,000 blog posts and put them into a spreadsheet.\\nThis process can vary depending on the tools and CMS you use. Here’s how we did it using Content Hub:\\n1. Log into HubSpot and navigate to the Blog page in Content Hub.\\n2. Navigate to the Actions drop-down and click Export blog posts.\\n3. Select File format and click Export. This will send all of your blog post information to your email. You will also get a notification in HubSpot once your export is ready.\\n4. Download your export and open it in your preferred spreadsheet software (I’m usually a Google Sheets girlie, but I had to use Microsoft Excel since the file was so large).\\n5. Review each column in the spreadsheet and delete the ones that are not relevant to your audit. We immediately deleted the following:\\n\\nPost SEO title\\nMeta description\\nLast modified date\\nPost body\\nFeatured image URL\\nHead HTML\\nArchived\\n\\n6. Once the irrelevant columns were removed, the following remained:\\n\\nBlog name\\nPost title\\nTags\\nPost language\\nPost URL\\nAuthor\\nPublish date\\nStatus\\n\\n7. Filter the Post language column for EN posts only. Delete the column once the sheet is filtered.\\n8. Filter the Status column for PUBLISHED only. Delete the column once the sheet is filtered.\\n9. Filter the sheet by Publish date from oldest to newest.\\n10. Highlight and copy the first 4,000 rows and paste them into a separate spreadsheet.\\n11. Name the new spreadsheet Content Audit Master.\\nIf you’re feeling fancy, you can also create a custom report in Content Hub and select only the fields you want included in the audit so you don’t have to filter as much when setting up your spreadsheet.\\n3. Retrieve the data.\\nAfter compiling all of the content needed for our audit, we had to collect relevant data for each blog post.\\nFor this audit, we kept it pretty simple, only analyzing total organic traffic from the previous calendar year, total backlinks, and total keywords.\\nWe did this because our recommended actions for each URL were determined during post-evaluation. (We’ll cover this in the next step.)\\nWe obtained organic traffic data from Google Search Console and used a VLOOKUP to match each URL with its corresponding number of Clicks.\\nThen, we got backlink and keyword data via a keyword research tool.\\n4. Evaluate the content.\\nNext, we assessed each piece of content by using the collected data. Then, we evaluated the post itself to determine the following:\\n\\nType of Content\\nFreshness Level\\nOrganic Potential\\n\\nType of Content\\nThe HubSpot Blog is home to many different types of blog posts, each serving a unique purpose. Labeling each post helped us determine its relevance and became a key factor in our decision to update or prune.\\nWhile this isn’t an exhaustive list of all the content types you could find on the HubSpot Blog, we narrowed it down to the following for the purposes of this audit:\\n\\nEducational: A topic that can educate the user on a pain point or problem they know they have.\\nThought Leadership: A topic that can educate the user on a pain point or problem they didn’t know they had until an expert drew it to their attention.\\nBusiness Update: A HubSpot-related piece of news or a press release that is likely not evergreen.\\nNewsjacking: An industry-related piece of news or a press release that is likely not evergreen.\\nResearch: A collection of data or the results of an experiment that is used to educate the reader. This topic may or may not be evergreen, but the content is not and needs updates to stay fresh.\\n\\nFreshness Level\\nBecause the posts in this audit hadn’t been updated in a long time, none of them could be considered 100% “fresh.” However, we took different types of freshness into account when determining what action needed to be taken on the URLs.\\nFor example, some topics, such as Google+, are so outdated that an update would be silly. However, plenty of topics were still evergreen, even though our content was not.\\nThe following scale helped us make decisions on whether the URL had value with regard to freshness:\\n\\nOutdated: The topic is outdated, and an update may not be possible.\\nStale: The topic is evergreen, but it would need an extensive update to make the content more competitive.\\nRelatively Fresh: The topic is evergreen, and it would only need a moderate update to make the content competitive.\\n\\nOrganic Potential\\nTo determine the organic potential of each URL, we had to ask ourselves the following question: Will anyone search for the content on Google?\\n\\nYes: Someone would definitely search for this, so we’ll need to optimize/recycle the content.\\nNo: Someone would not search for this. There’s no point in optimizing/recycling the content since there’s no possible focus keyword.\\n\\nFor all of the posts marked “Yes” for organic potential, we recommended a focus keyword for the re-optimized content to compete for. We did this by evaluating the existing title, slug, and content. Then, we did some keyword research and reviewed the Google SERP for that query.\\nWe also included the focus keyword’s monthly search volume (MSV) to help prioritize which updates to perform first.\\xa0\\nFor an extra layer of caution, we also checked for cannibalization on all posts marked “Yes” for organic potential. There are a few ways to do this:\\n\\nDo a site search and see if any URLs come up for the focus keyword.\\nUse a keyword research tool to cross-reference whether any other posts on your site are ranking. (This is the fastest/most accurate method.)\\n\\nIf the focus keyword was flagged for cannibalization, we either found a different focus keyword or noted that the URL should be redirected to the fresher post.\\nIf no cannibalization was found, then we had the green light to move forward with updating the post.\\n5. Recommend an action.\\nOnce a post was completely evaluated, we turned the insights into action items.\\nEach URL was placed into one of the following categories:\\n\\nKeep: No action is needed because both the content and the URL are good.\\nOptimize: The content is good but outdated in terms of freshness or SEO practices. Keep the spirit of the article, but refresh and re-optimize to improve performance.\\nRecycle: The content is not salvageable, but the URL still has value (in terms of backlinks or keyword opportunity). Create new content from scratch, but retain the URL.\\nPrune: Neither the content nor the URL has value from an organic standpoint.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHubSpot's Guide to Navigating EEAT in the AI Era\\nDiscover how HubSpot tackled SERP volatility and redefined its strategy to thrive in the new EEAT landscape. \\n\\n\\n\\n\\n\\n              Strategies for creating experience-rich & authoritative content\\n            \\n\\n\\n\\n\\n              Insights from HubSpot experts \\n            \\n\\n\\n\\n\\n              Practical tools to implement EEAT\\n            \\n\\nGet the Guide\\n\\n                Learn more\\n                \\n\\n\\n\\n\\n\\n                Get the Guide\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDownload FreeAll fields are required.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou're all set!\\nClick this link to access this resource at any time.\\n\\nDownload Now\\n\\n\\n\\n\\n\\n\\nAudit Insights\\nOut of the 4,000 URLs we audited, 951 (23.78%) were categorized as posts with organic potential and recommended for optimization or recycling. Additionally, 2,888 URLs were recommended to be pruned. That’s about 72.2% of the audit.\\nThese posts either did not have organic potential, posed a cannibalization risk, or were so outdated that there was no point in updating them.\\nThe remaining 161 URLs either did not require any action or had already been redirected.\\nHow We Took Action\\nThe action taken for a URL was determined by its potential for organic traffic.\\nThe URLs with organic potential were delivered to our Blog team and recommended to be optimized or recycled.\\nMeanwhile, the URLs with no organic potential were delivered to our SEO team and recommended to be archived or redirected.\\nFirst, let’s walk through how we took action on the posts recommended to be optimized or recycled.\\nTaking Action on Content with Organic Potential\\nBefore addressing any of the 951 posts with organic potential, we needed to figure out the following:\\n\\nOur capacity for strategic analysis and brief writing\\nThe capacity of our in-house writing staff and available freelancers\\nOur capacity to edit the updates\\n\\nWe coordinated with stakeholders and determined we only had the bandwidth to update 240 posts in 2024 (in addition to the dozens of blog posts we update each month). This initiative was internally known as the “De-Age the Blog Project” and was led by my EN Blog Strategy teammate Kimberly Turner.\\nOnce we knew how many posts we could take on, we had to narrow down which ones to prioritize. We did this by evaluating the complexity of the lift required for each post update:\\n\\nSimple Update: The content updates needed are relatively light, making them suitable for freelancers.\\nComplex Update: The content updates needed are heavy, making them better suited for in-house writers.\\nRecycle: Content is not salvageable, but the URL is. Rewrite the post from scratch, but retain the URL.\\nNo Opportunity: Pass on updating.\\n\\nWe originally prioritized updating the simplest URLs first, but later pivoted our strategy to tackle the URLs with the highest MSV potential, regardless of update complexity.\\nWe did this because we wanted to get the most we could out of our updates.\\nDe-Age the Blog Results\\nInitially, we projected that these updates would be complete by the end of H1 2024, but we had to shift our strategy … again.\\nLike many other publishers, we felt the effects of Google’s March 2024 Core Update as well as the introduction of AI Overviews.\\nAfter having placed the De-Age the Blog Project on hold while we addressed the issues, we deprioritized the project entirely in favor of higher-impact workstreams.\\nSEO, am I right? It always keeps you on your toes.\\nDespite sunsetting the project before it was complete, we were still able to perform 76 post updates. Six months after the updates were implemented, the cumulative monthly traffic for these posts had increased by 458%.\\nThis goes to show that even updating a small portion of URLs can make a big difference.\\nTaking Action on Content with No Organic Potential\\nWhile the De-Age the Blog Project was taking place, we also took action on the 2,888 URLs that were recommended to be pruned.\\nSince the initial audit didn’t include recommendations on how to prune, we had to go back and re-review each URL to determine how we would prune.\\nHere’s how we evaluated the posts:\\n\\nArchive (404): The URL has less than 10 backlinks and the backlink profile does not have value.\\nRedirect (301): The URL has more than 10 backlinks and/or the backlink profile has value.\\n\\nWhen choosing a new URL, we did our best to pick the most relevant and similar page. If we couldn’t find one, we redirected to the pillar page of the cluster that the post belonged to.\\nIf for some reason, the URL didn’t belong to a cluster or there wasn’t a pillar page, we redirected it to the HubSpot Blog homepage.\\nDecision-making for some content types was easier than others. For example, we were able to automatically assign 301 redirects to URLs that were flagged for cannibalization during the initial audit. We also automatically assigned 404s to URLs with less than 10 backlinks labeled as Newsjacking and Business Updates.\\nEverything else was manually reviewed to ensure accuracy. To make the evaluation process easier, we followed this decision tree:\\n\\nIt took my team about two and a half weeks to ensure that every URL had the correct label. In the end, we had 1,675 URLs assigned to be redirected and 1,210 URLs assigned to be unpublished and archived.\\nOnce each URL was evaluated, we were finally ready to take action.\\nAfter coordinating with Rory and Principal Technical SEO Strategist, Sylvain Charbit, we decided to prune the URLs in batches instead of all at once. That way, we could better monitor the impact of redirecting and archiving a large quantity of content.\\nOriginally, we planned to implement our prune in five batches over five weeks, allowing us time to monitor performance during the weeks in between.\\nBatches one and two contained URLs meant to be archived and unpublished, and batches three through five contained URLs designated for 301 redirects.\\nBecause there were so many URLs to unpublish and archive, we worked with developers on HubSpot’s Digital Experience team to create a script that would automatically unpublish and archive URLs and redirect them to our 404 page.\\nThen, we were able to implement the 301 redirects with the Bulk URL Redirect tool in Content Hub.\\nNote: Although we were able to work through this process internally and finish before our deadline, I want to acknowledge that manually evaluating over 2,000 URLs can be tedious and time-consuming.\\nDepending on your resources and the scope of your audit, you may want to consider hiring a freelancer to help your team work through a task this large.\\nContent Pruning Results\\nWhile we successfully implemented each batch, this process didn’t come without a few road bumps.\\nMidway through our pruning schedule, Google rolled out the March 2024 Core Algorithm Update. We ended up placing our pruning schedule on hold so we could better monitor performance during the update.\\nOnce the update was complete, we resumed the rest of our prune until it was complete.\\nBecause of the volatile search landscape in 2024, we didn’t see the traffic gains we’d hoped to see once the prune was complete. However, we did celebrate a massive win for overall content freshness on the blog.\\nAt the start of our audit in 2023, we calculated the freshness of our content library by looking at each URL's publish date and quantifying the number of days since they were updated.\\nFor example, say the current date is November 12, 2024, and you have a post that was last updated on February 19, 2008. Based on the 2024 date, the post from 2008 is 16.7 years old or 6,110 days.\\nOnce we had all of the ages for every post on the HubSpot Blog, we averaged those numbers to determine the average age of our content library, which was 2,088 days (5.7 years).\\nSince pruning 2,888 URLs (and updating hundreds of URLs from the audit and beyond), the HubSpot Blog's average age has dropped to 1,747 days — that’s 341 days younger than when we started.\\nAs content freshness and helpfulness play an even greater role in search algorithms, being nearly a year younger can make a big difference.\\nWhat’s Next?\\nEarlier in this post, I mentioned that this audit is only one of three that my team has worked on in 2024.\\nOur phase two audit focuses on the lowest-performing posts that were not included in phase one, totaling over 6000 URLs. Then, phase three assesses the value of our Blog’s topic clusters.\\nWe’re still taking action on the results from these audits, but I’m so excited to share the process and insights when they’re complete.\\nUltimately, content auditing is a job that is never truly done — especially when working with large libraries. You finish one audit, then it’s on to the next.\\nAlthough the work can be tedious, the rewards of improving content quality, user experience, and performance make it worth the effort.\\n\\n\\nTopics:\\nSEO\\n\\n\\n\\nDon't forget to share this post!\",\n",
       " 'word_count': 3348}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01385fd9",
   "metadata": {},
   "source": [
    "# Data Transformation and Cleaning\n",
    "\n",
    "Takes the raw scraped articles and cleans them up for better training data. The clean_article_content function removes unwanted promotional text like \"Download Free\" messages, subscription prompts, and duplicate whitespace that clutters the content. The remove_duplicates function finds articles with identical content by comparing the first 200 characters of each article and keeps only unique ones. Goes through each scraped article, applies the cleaning function, and skips articles that become too short after cleaning. Creates new clean article dictionaries with updated word counts. Finally removes any duplicate articles and shows how many articles survived the cleaning process compared to the original count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce882cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 23 articles\n",
      "After cleaning: 20 articles\n"
     ]
    }
   ],
   "source": [
    "def clean_article_content(content):\n",
    "    \"\"\"Clean messy article content\"\"\"\n",
    "    if not content:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove common junk patterns\n",
    "    patterns_to_remove = [\n",
    "        r'Download.*?Free.*?All fields are required.*?Download Now',\n",
    "        r'Get.*?Free.*?Learn more.*?Get.*?Free',\n",
    "        r'Topics:.*?Don\\'t forget to share this post!',\n",
    "        r'Most Popular.*?Updated.*?\\d+/\\d+/\\d+',\n",
    "        r'Source.*?Why I Think This Marketing Plan Works',\n",
    "        r'\\n\\s*\\n\\s*\\n+',  # Multiple blank lines\n",
    "    ]\n",
    "    \n",
    "    cleaned = content\n",
    "    for pattern in patterns_to_remove:\n",
    "        cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def remove_duplicates(articles):\n",
    "    \"\"\"Remove duplicate articles\"\"\"\n",
    "    seen_content = set()\n",
    "    unique_articles = []\n",
    "    \n",
    "    for article in articles:\n",
    "        # Use first 200 chars as fingerprint\n",
    "        fingerprint = article['content'][:200].strip()\n",
    "        \n",
    "        if fingerprint not in seen_content:\n",
    "            seen_content.add(fingerprint)\n",
    "            unique_articles.append(article)\n",
    "    \n",
    "    return unique_articles\n",
    "\n",
    "# Clean your existing data\n",
    "cleaned_articles = []\n",
    "\n",
    "for article in all_articles:\n",
    "    cleaned_content = clean_article_content(article['content'])\n",
    "    \n",
    "    # Skip if too short after cleaning\n",
    "    if len(cleaned_content) < 500:\n",
    "        continue\n",
    "        \n",
    "    cleaned_article = {\n",
    "        'url': article['url'],\n",
    "        'title': article['title'].strip(),\n",
    "        'content': cleaned_content,\n",
    "        'word_count': len(cleaned_content.split())\n",
    "    }\n",
    "    \n",
    "    cleaned_articles.append(cleaned_article)\n",
    "\n",
    "# Remove duplicates\n",
    "final_articles = remove_duplicates(cleaned_articles)\n",
    "\n",
    "print(f\"Original: {len(all_articles)} articles\")\n",
    "print(f\"After cleaning: {len(final_articles)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a14eb75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1:\n",
      "Title: How Content Audits Help The HubSpot Blog Age Backwards — A Peek Into Our Process\n",
      "Word count: 2247\n",
      "Content preview: In 2023, my team and I began working on perhaps one of the most ambitious content audits ever conducted on the HubSpot Blog. We’ve run content audits in the past — but not like this. We ran the audit ...\n",
      "--------------------------------------------------\n",
      "Article 2:\n",
      "Title: How to write an effective communication plan [+ templates]\n",
      "Word count: 704\n",
      "Content preview: Free Communications Plan Template A structured framework to help you craft, execute, and refine an effective communication strategy. Communication Objectives Stakeholder Analysis Messaging Strategy Im...\n",
      "--------------------------------------------------\n",
      "Article 3:\n",
      "Title: What is a marketing plan & how to write one [+ examples]\n",
      "Word count: 809\n",
      "Content preview: Free Marketing Plan Template Outline your company's marketing strategy in one simple, coherent plan. Pre-Sectioned Template Completely Customizable Example Prompts Professionally Designed The 2025 Sta...\n",
      "--------------------------------------------------\n",
      "\n",
      "Word count stats:\n",
      "Shortest: 107 words\n",
      "Longest: 7239 words\n",
      "Average: 1960 words\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "for i, article in enumerate(final_articles[:3], 1):\n",
    "    print(f\"Article {i}:\")\n",
    "    print(f\"Title: {article['title']}\")\n",
    "    print(f\"Word count: {article['word_count']}\")\n",
    "    print(f\"Content preview: {article['content'][:200]}...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Check the shortest and longest articles\n",
    "word_counts = [a['word_count'] for a in final_articles]\n",
    "print(f\"\\nWord count stats:\")\n",
    "print(f\"Shortest: {min(word_counts)} words\")\n",
    "print(f\"Longest: {max(word_counts)} words\")\n",
    "print(f\"Average: {sum(word_counts) // len(word_counts)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38472596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles under 1000 words: 6\n",
      "- How to write an effective communication plan [+ templates]: 704 words\n",
      "- What is a marketing plan & how to write one [+ examples]: 809 words\n",
      "- How to get more followers on Instagram: 17 ways to your first (or next) 1000: 107 words\n",
      "- The top search engines other than Google [+ some you might not expect]: 816 words\n",
      "- Why creator marketing works for any business [Tips from a creator consultant]: 892 words\n",
      "- The 2025 State of Marketing & Trends Report: Data from 1700+ Global Marketers: 343 words\n",
      "\n",
      "Filtered to 14 substantial articles\n",
      "Average word count: 2538 words\n"
     ]
    }
   ],
   "source": [
    "# Check articles under 1000 words\n",
    "short_articles = [a for a in final_articles if a['word_count'] < 1000]\n",
    "print(f\"Articles under 1000 words: {len(short_articles)}\")\n",
    "\n",
    "for article in short_articles:\n",
    "    print(f\"- {article['title']}: {article['word_count']} words\")\n",
    "\n",
    "# Keep only substantial articles (1000+ words)\n",
    "substantial_articles = [a for a in final_articles if a['word_count'] >= 1000]\n",
    "\n",
    "print(f\"\\nFiltered to {len(substantial_articles)} substantial articles\")\n",
    "print(f\"Average word count: {sum(a['word_count'] for a in substantial_articles) // len(substantial_articles)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5171a13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 14 high-quality marketing articles!\n",
      "Total words: 35,541\n",
      "Ready for LLM training!\n"
     ]
    }
   ],
   "source": [
    "# Save the final cleaned dataset\n",
    "with open('marketing_articles.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(substantial_articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Saved 14 high-quality marketing articles!\")\n",
    "print(f\"Total words: {sum(a['word_count'] for a in substantial_articles):,}\")\n",
    "print(\"Ready for LLM training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c0c4c",
   "metadata": {},
   "source": [
    "# Create chat format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b89de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 14 conversations for Llama fine-tuning\n",
      "Saved as llama_marketing_training.jsonl\n",
      "\n",
      "Example conversation:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are a marketing expert who creates engaging marketing content.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Write marketing content about: How Content Audits Help The HubSpot Blog Age Backwards \\u2014\\u00a0A Peek Into Our Process\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"In 2023, my team and I began working on perhaps one of the most ambitious content audits ever conducted on the HubSpot Blog. We\\u201...\n"
     ]
    }
   ],
   "source": [
    "def create_llama_training_data(articles):\n",
    "    \"\"\"Create training data in Llama chat format\"\"\"\n",
    "    training_data = []\n",
    "    \n",
    "    for article in articles:\n",
    "        conversation = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a marketing expert who creates engaging marketing content.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"Write marketing content about: {article['title']}\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": article['content'][:1500]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        training_data.append(conversation)\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "llama_data = create_llama_training_data(substantial_articles)\n",
    "\n",
    "with open('llama_marketing_training.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in llama_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Created {len(llama_data)} conversations for Llama fine-tuning\")\n",
    "print(\"Saved as llama_marketing_training.jsonl\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample conversation:\")\n",
    "print(json.dumps(llama_data[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac36ebe",
   "metadata": {},
   "source": [
    "# Data Quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7cec580e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response length: ~272 words\n"
     ]
    }
   ],
   "source": [
    "with open('llama_marketing_training.jsonl', 'r') as f:\n",
    "    first_conversation = json.loads(f.readline())\n",
    "\n",
    "word_count = len(first_conversation['messages'][2]['content'].split())\n",
    "print(f\"Average response length: ~{word_count} words\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
